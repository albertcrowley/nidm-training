# NIDM files
There are two files that will be of interest to developers working with NIDM datasets. The first is the nidm.ttl. This file is a RDF document it "turtle" format describing the metadata for the associated dataset. The second file is the nidm.ttl.png file. This image file show a graphical representation of the RDF graph for the dataset. It may be useful to see at a glance what information is available for each entity.

At the top of the nidm.ttl file you will see a list of namespace prefixes used to provide short names for elements used frequently in the rest of the file. When querying the NIDM file using SPARQL you will typically use the prefix + the identifier. For example, at the top of the file you may see the prefix:

	@prefix ncicb: <http://ncicb.nci.nih.gov/xml/owl/EVS/Thesaurus.owl#> .
   
Later in the document you will then see that prefix used when defining the age of an entity:

      ncicb:Age "0.876"^^xsd:string ;
      
When refering to the Age of that entity in a SPARQL query you would refere to it as "ncicb:Age"

# Searching and Querying NIDM files
It is _possible_ to parse and navigate the nidm.ttl file using PyNIDM but this can be difficult. A better option is to use something like RDFLib to write SPARQL queries to report on the data.

One concept that will make writing SPARQL queries a bit easier is understanding that the RDF file is structured as a set of "triles." A triple consists of a subject-predicate-object. For example the triple "X, was generated by, Y" or "subject X, has the age of, 7". 


# NIDM Development Setup

# Vagrant VM
The Vagrant file provided in this repository can be used to launch a VirtualBox VM compatible with the instructions below. It may take a long time when first started as a number of packages will be installed. Be prepared for 30-60 minutes. If you don't need a  graphical interface and can be happy with a terminal window, open the Vagrant file and comment out out the apt-get command for installing kubuntu-desktop. To startup the VM, install [VirtualBox](https://www.virtualbox.org/wiki/Downloads) and [Vagrant](https://www.vagrantup.com/downloads.html). Then navigate in a terminal window to the directory containing this repo and type:

	vagrant up

This will install and launch a new Ubuntu VM. When the provisioning is complete, **the VM will reboot so you may want to wait for that to complete before logging in.** 

You can also use any recent linux distribution to run thgouth these instructions. It should work for OSX as well, but hasn't yet been tested there. If using a non-Ubuntu OS you may need to adapt the pacakage installation commands to work with your system. Some older Linux OS may not be able to install all the required packages using the built in package manager. 


## Install python3
Best to use python 3 for future compatibility. 

	  sudo apt-get install python3

## Install Anaconda
Use a conda environment so packages won't conflict with other applications.

	  cd ~/Downloads
	  wget https://repo.anaconda.com/archive/Anaconda3-5.1.0-Linux-x86_64.sh
	  sudo ./Anaconda3-5.1.0-Linux-x86_64.sh
	  echo 'export PATH=/home/vagrant/anaconda3/bin:$PATH' >> ~/.bashrc
	  source ~/.bashrc


## Setup conda environment
This will create a pynidm_py3 conda environment and install the packages needed to use PyNIDM and the RDFLib to query nidm.ttl files.

	cd ~/
	sudo apt-get -y install graphviz
	git clone https://github.com/incf-nidash/PyNIDM.git
	conda create -n pynidm_py3 python=3 pytest graphviz -y
	source activate pynidm_py3
	cd PyNIDM
	pip install -e .
	pytest
	pip install rdflib requests fuzzywuzzy owlready2 pygithub pybids duecredit

## Download a datasets
Starting with a BIDS formatted dataset you can create a nidm.ttl file. You can find free/open datasets at OpenNeuro. An example is the [Indiv_Diffs_ReadingSkill](https://openneuro.org/datasets/ds001365/versions/00001) dataset. On the dataset details page you will need to click the small download icon to download the dataset. I didn't see the icon at first. It looks like ![download icon](https://raw.githubusercontent.com/albertcrowley/nidm-training/master/download-icon.png).   

Save the downloaded file in ~/Downloads. If you are following along with the instructions you should unzip the Indiv_Diffs_ReadingSkill dataset in ~/workspace

	mkdir ~/workspace
	cd ~/workspace
	tar -xvf ~/Downloads/Indiv*.tar

## Add NIDM data to CMU_b dataset
The BIDSMRI2NIDM.py script from teh PyNIDM repository will create the nidm.ttl and nidm.ttl.png files for a valid BIDS dataset. You can run it with:

      cd ~/workspace/Indiv_Diffs_ReadingSkill
      ~/PyNIDM/bin/BIDSMRI2NIDM.py -d ~/workspace/Indiv_Diffs_ReadingSkill

## Run sample SPARQL query
You are now ready to query the dataset using a simple SPARQL query in the rdf-agey-query.py sample script. This script will return the subject IDs, age of each subject, and the assessment ID from the dataset. You can try it with:
   cd ~/
   git clone https://github.com/albertcrowley/nidm-training.git
   cd nidm-training
   python rdf-age-query.py -nidm ~/workspace/Indiv_Diffs_ReadingSkill/nidm.ttl

If you read the rdf-age-query.py script, you will see the SPARQL query:
	
	SELECT DISTINCT ?id ?age ?assessment   
	WHERE {
	  ?assessment prov:wasGeneratedBy ?acq .
	  ?acq prov:wasAssociatedWith ?person .
	  ?assessment ncicb:Age ?age .
	  ?person ndar:src_subject_id ?id
	}
	
Most of the complication in this query is because you must link the age and subject ID through related assessments and acquisitions. When viewing the results you will see that that the assessment ID is a NIDM identifier and not any ID found in the underlying BIDS dataset. The person/subject id _is_ an identifier used in the original dataset and is associated with the NIDM entity by "ndar:src_subject_id" which is a predicate meaning "has subject id".
